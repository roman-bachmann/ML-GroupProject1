{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Project 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import proj1_helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations of ML Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 2, 3, 4])\n",
    "tx = np.array([[1, 2, 3], \n",
    "               [4, 5, 6], \n",
    "               [7, 8, 9], \n",
    "               [10, 11, 12]])\n",
    "w = np.array([0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_mse(y, tx, w):\n",
    "    \"\"\"Calculate the MSE loss.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx, w)\n",
    "    return np.dot(e,e) / (2 * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_mae(y, tx, w):\n",
    "    \"\"\"Calculate the MAE loss.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx, w)\n",
    "    return np.sum(np.absolute(e)) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(y, tx, w):\n",
    "    \"\"\"Computes the Root Mean Square Error\"\"\"\n",
    "    mse = compute_loss_mse(y, tx, w)\n",
    "    return np.sqrt(2 * mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_mse(y, tx, w):\n",
    "    \"\"\"Compute the MSE gradient.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx, w)\n",
    "    return (-1/N) * np.dot(np.transpose(tx), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stochastic_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a stochastic subgradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx, w)\n",
    "    abs_e_subgrad = [np.sign(en) for en in e] # Sign chosen for subgradient of absolute value function\n",
    "    return (-1/N) * np.dot(np.transpose(tx), abs_e_subgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    np.random.seed(seed) # set seed\n",
    "    permuted_idxs = np.random.permutation(x.shape[0])\n",
    "    train_size = int(ratio * x.shape[0])\n",
    "    train_idxs, test_idxs = permuted_idxs[:train_size], permuted_idxs[train_size:]\n",
    "    \n",
    "    return x[train_idxs], x[test_idxs], y[train_idxs], y[test_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to implement for project 1 submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Linear regression using gradient descent\"\"\"\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm using MSE.\"\"\"\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient_mse(y, tx, w)\n",
    "        loss = compute_loss_mse(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "    rmse = compute_rmse(y, tx, w)\n",
    "\n",
    "    return w, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Linear regression using stochastic gradient descent\"\"\"\n",
    "def stochastic_subgradient_descent_mae(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Stochastic subgradient descent algorithm using MAE.\"\"\"\n",
    "    batch_size = 1\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            g = compute_stochastic_subgradient_mae(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * g\n",
    "        loss = compute_loss_mae(y, tx, w)\n",
    "        print(\"Stochastic Subgradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculates the explicit least squares solution.\n",
    "    Returns rmse, optimal weights\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    D = tx.shape[1]\n",
    "    rank_tx = np.linalg.matrix_rank(tx)\n",
    "    \n",
    "    # Check if tx is invertible. If so, find explicit solution\n",
    "    # using real inverses.\n",
    "    # If not, find explicit solution using pseudoinverses.\n",
    "    if (rank_tx == max(tx.shape[0], tx.shape[1])):\n",
    "        gramian_inv = np.linalg.inv(np.dot(tx.T, tx))\n",
    "        w = np.dot(gramian_inv, np.dot(tx.T, y))\n",
    "    else:\n",
    "        U, s, V_T = np.linalg.svd(tx)\n",
    "        S_inv_T = np.zeros((D, N))\n",
    "        S_inv_T[:len(s), :len(s)] = np.diag(1/s)\n",
    "        w = np.dot(V_T.T, np.dot(S_inv_T, np.dot(U.T, y)))\n",
    "    \n",
    "    rmse = compute_rmse(y, tx, w)\n",
    "    \n",
    "    return w, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"Ridge regression using normal equations\"\"\"\n",
    "    \n",
    "    #if (lambda_ == 0):\n",
    "    #    return least_squares(y, tx)\n",
    "    \n",
    "    N = tx.shape[0]\n",
    "    D = tx.shape[1]\n",
    "    \n",
    "    inv = np.linalg.inv(np.dot(tx.T, tx) + 2 * N * lambda_ * np.identity(D))\n",
    "    w = np.dot(inv, np.dot(tx.T, y))\n",
    "    \n",
    "    rmse = compute_rmse(y, tx, w)\n",
    "    \n",
    "    return w, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Logistic regression using gradient descent or SGD\"\"\"\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Regularized logistic regression using gradient descent or SGD\"\"\"\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higgs Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "data_path = \"./data/train.csv\"\n",
    "yb, input_data, ids = load_csv_data(data_path, sub_sample=False)\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(input_data, yb, ratio=0.9, seed=123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness(yb, y_pred):\n",
    "    correct = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if (y_pred[i] == yb[i]):\n",
    "            correct += 1\n",
    "        \n",
    "    incorrect = len(y_pred) - correct\n",
    "    perc = correct / len(y_pred) * 100\n",
    "    print(\"Total correct:\", correct, \"\\nTotal incorrect:\", incorrect, \"\\nCorrect percentage:\", perc, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "weights, rmse = ridge_regression(yb, input_data, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(weights, input_data)\n",
    "create_csv_submission(ids, y_pred, \"test_output\")\n",
    "correctness(yb, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Builds polynomial basis function for input data matrix tx, for j=0 up to j=degree,\n",
    "where the result will be a matrix of form [1, tx, tx^2, ..., tx^j]\n",
    "\"\"\"\n",
    "\n",
    "def build_simple_poly(tx, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((tx.shape[0], 1))\n",
    "\n",
    "    for j in range(1, degree+1):\n",
    "        poly = np.column_stack((poly, np.power(tx, j)))\n",
    "\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def build_mult_combinations(tx, deg):\n",
    "    N = tx.shape[0]\n",
    "    comb_iter = itertools.combinations_with_replacement(range(tx.shape[1]), deg)\n",
    "    mult = np.empty((N,0))\n",
    "    for comb in comb_iter:\n",
    "        mult_col = np.ones((N, 1))\n",
    "        for idx in comb:\n",
    "            tx_col = tx[:,idx].reshape(N,1)\n",
    "            mult_col = np.multiply(mult_col, tx_col)\n",
    "        mult = np.append(mult, mult_col, axis=1)\n",
    "    return mult\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "Builds polynomial basis function for input data matrix tx, for j=0 up to j=degree,\n",
    "where the result will be a matrix of form \n",
    "[1, tx, comb_mult(tx, 2), ..., comb_mult(tx, j)]\n",
    "\"\"\"\n",
    "def build_advanced_poly(tx, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((tx.shape[0], 1))\n",
    "\n",
    "    for j in range(1, degree+1):\n",
    "        mult = build_mult_combinations(tx, j)\n",
    "        poly = np.column_stack((poly, mult))\n",
    "\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_poly = build_simple_poly(input_data, degree=2)\n",
    "x_train_poly = build_simple_poly(x_train, degree=2)\n",
    "x_test_poly = build_simple_poly(x_test, degree=2)\n",
    "\n",
    "weights, rmse = ridge_regression(y_train, x_train_poly, lambda_=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct: 192608 \n",
      "Total incorrect: 57392 \n",
      "Correct percentage: 77.0432 %\n",
      "-----------------------------\n",
      "Train RMSE: 0.797452588264 , Test RMSE: 0.796328693096\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(weights, x_poly)\n",
    "correctness(yb, y_pred)\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "rmse_train = compute_rmse(y_train, x_train_poly, weights)\n",
    "rmse_test = compute_rmse(y_test, x_test_poly, weights)\n",
    "print(\"Train RMSE:\", rmse_train, \", Test RMSE:\", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
