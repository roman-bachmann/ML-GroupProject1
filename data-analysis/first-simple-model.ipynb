{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Project 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import proj1_helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations of ML Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data:\n",
    "\n",
    "Testing data for helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 2, 3, 4])\n",
    "tx = np.array([[1, 2, 3], \n",
    "               [4, 5, 6], \n",
    "               [7, 8, 9], \n",
    "               [10, 11, 12]])\n",
    "w = np.array([0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_mse(y, tx, w):\n",
    "    \"\"\"Calculate the MSE loss.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx, w)\n",
    "    return np.dot(e,e) / (2 * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_mae(y, tx, w):\n",
    "    \"\"\"Calculate the MAE loss.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx, w)\n",
    "    return np.sum(np.absolute(e)) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(y, tx, w):\n",
    "    \"\"\"Computes the Root Mean Square Error\"\"\"\n",
    "    mse = compute_loss_mse(y, tx, w)\n",
    "    return np.sqrt(2 * mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Filippas\"\"\"\n",
    "def rmse_calc(mse):\n",
    "    rmse = np.sqrt(mse*2)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_mse(y, tx, w):\n",
    "    \"\"\"Compute the MSE gradient.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx, w)\n",
    "    return (-1/N) * np.dot(np.transpose(tx), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stochastic_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a stochastic subgradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx, w)\n",
    "    abs_e_subgrad = [np.sign(en) for en in e] # Sign chosen for subgradient of absolute value function\n",
    "    return (-1/N) * np.dot(np.transpose(tx), abs_e_subgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    np.random.seed(seed) # set seed\n",
    "    permuted_idxs = np.random.permutation(x.shape[0])\n",
    "    train_size = int(ratio * x.shape[0])\n",
    "    train_idxs, test_idxs = permuted_idxs[:train_size], permuted_idxs[train_size:]\n",
    "    \n",
    "    return x[train_idxs], x[test_idxs], y[train_idxs], y[test_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to implement for project 1 submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Linear regression using gradient descent\"\"\"\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm using MSE.\"\"\"\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient_mse(y, tx, w)\n",
    "        loss = compute_loss_mse(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "    rmse = compute_rmse(y, tx, w)\n",
    "\n",
    "    return w, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Linear regression using stochastic gradient descent\"\"\"\n",
    "def stochastic_subgradient_descent_mae(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Stochastic subgradient descent algorithm using MAE.\"\"\"\n",
    "    batch_size = 1\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            g = compute_stochastic_subgradient_mae(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * g\n",
    "        loss = compute_loss_mae(y, tx, w)\n",
    "        print(\"Stochastic Subgradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculates the explicit least squares solution.\n",
    "    Returns rmse, optimal weights\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    D = tx.shape[1]\n",
    "    rank_tx = np.linalg.matrix_rank(tx)\n",
    "    \n",
    "    # Check if tx is invertible. If so, find explicit solution\n",
    "    # using real inverses.\n",
    "    # If not, find explicit solution using pseudoinverses.\n",
    "    if (rank_tx == max(tx.shape[0], tx.shape[1])):\n",
    "        gramian_inv = np.linalg.inv(np.dot(tx.T, tx))\n",
    "        w = np.dot(gramian_inv, np.dot(tx.T, y))\n",
    "    else:\n",
    "        U, s, V_T = np.linalg.svd(tx)\n",
    "        S_inv_T = np.zeros((D, N))\n",
    "        S_inv_T[:len(s), :len(s)] = np.diag(1/s)\n",
    "        w = np.dot(V_T.T, np.dot(S_inv_T, np.dot(U.T, y)))\n",
    "    \n",
    "    rmse = compute_rmse(y, tx, w)\n",
    "    \n",
    "    return w, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Filippas\"\"\"\n",
    "def least_squares_2(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    w = np.linalg.solve(np.dot(tx.T, tx), np.dot(tx.T, y))\n",
    "    \n",
    "    e = y-np.dot(tx, w)\n",
    "    mse = (e ** 2).mean()\n",
    "    return w, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"Ridge regression using normal equations\"\"\"\n",
    "    \n",
    "    #if (lambda_ == 0):\n",
    "    #    return least_squares(y, tx)\n",
    "    \n",
    "    N = tx.shape[0]\n",
    "    D = tx.shape[1]\n",
    "    \n",
    "    inv = np.linalg.inv(np.dot(tx.T, tx) + 2 * N * lambda_ * np.identity(D))\n",
    "    w = np.dot(inv, np.dot(tx.T, y))\n",
    "    \n",
    "    rmse = compute_rmse(y, tx, w)\n",
    "\n",
    "    return w, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    " def ridge_regression2(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    D = tx.shape[1]\n",
    "    inver = np.linalg.inv(np.dot(tx.T, tx) + (2 * N * lambda_ * np.identity(D)))\n",
    "    w = np.dot(inver, np.dot(tx.T,y))\n",
    "    mse = compute_loss_mse(y, tx, w)\n",
    "    rmse = rmse_calc(mse)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression: TODO\n",
    "    # ***************************************************\n",
    "    return w, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Logistic regression using gradient descent or SGD\"\"\"\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Regularized logistic regression using gradient descent or SGD\"\"\"\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higgs Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental functions\n",
    "\n",
    "All of these functions can be used experimentally to, in one way or the other, improve the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness(yb, y_pred):\n",
    "    \"\"\"Takes inputs known y and predicted y and prints the ratio of correct predictions vs incorrect ones.\"\"\"\n",
    "    correct = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if (y_pred[i] == yb[i]):\n",
    "            correct += 1\n",
    "        \n",
    "    incorrect = len(y_pred) - correct\n",
    "    perc = correct / len(y_pred) * 100\n",
    "    print(\"Total correct:\", correct, \"\\nTotal incorrect:\", incorrect, \"\\nCorrect percentage:\", perc, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_by_mean(tx, nan_value):\n",
    "    \"\"\"Replaces values with a specified nan_value by the column mean.\"\"\"\n",
    "    tx[tx == nan_value] = np.nan\n",
    "    col_mean = np.nanmean(tx, axis=0)\n",
    "    return np.where(np.isnan(tx), col_mean, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_by_median(tx, nan_value):\n",
    "    \"\"\"Replaces values with a specified nan_value by the column median.\"\"\"\n",
    "    tx[tx == nan_value] = np.nan\n",
    "    col_median = np.nanmedian(tx, axis=0)\n",
    "    return np.where(np.isnan(tx), col_median, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nan_rows(tx, nan_value):\n",
    "    \"\"\"Drop all rows that contain a nan equaling the specified nan_value.\"\"\"\n",
    "    tx[tx == nan_value] = np.nan\n",
    "    return tx[~np.isnan(tx).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_poly(tx, degree):\n",
    "    \"\"\"\n",
    "    Builds simple polynomial basis function for input data matrix tx, for j=0 up to j=degree,\n",
    "    where the result will be a matrix of form [1, tx, tx^2, ..., tx^j].\n",
    "    tx^j denotes that for each x_i,k in tx, the result will be (x_i,k)^j\n",
    "    \"\"\"\n",
    "    poly = np.ones((tx.shape[0], 1))\n",
    "\n",
    "    for j in range(1, degree+1):\n",
    "        poly = np.column_stack((poly, np.power(tx, j)))\n",
    "\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# TODO: Optimize. Very slow!\n",
    "def build_mult_comb(tx, deg, cols=[]):\n",
    "    \"\"\"\n",
    "    Returns all multiplicative combinations of the specified columns for degree deg.\n",
    "    For len(col) = D', there are (D' choose deg) combinations of columns that get\n",
    "    returned as a matrix.\n",
    "    If cols is not given, it returns the combinations of all columns of tx.\n",
    "    \"\"\"\n",
    "    N = tx.shape[0]\n",
    "    if (cols == []):\n",
    "        comb_iter = itertools.combinations_with_replacement(range(tx.shape[1]), deg)\n",
    "    else:\n",
    "        comb_iter = itertools.combinations_with_replacement(cols, deg)\n",
    "    mult = []\n",
    "    for comb in comb_iter:\n",
    "        mult_col = np.ones(N)\n",
    "        for idx in comb:\n",
    "            tx_col = tx[:,idx]\n",
    "            mult_col = np.multiply(mult_col, tx_col)\n",
    "        mult.append(mult_col.tolist())\n",
    "    return np.array(mult).T\n",
    "\n",
    "def build_advanced_poly(tx, degree, cols=[]):\n",
    "    \"\"\"\n",
    "    Builds full polynomial basis function for input data matrix tx, for j=0 up to j=degree,\n",
    "    where the result will be a matrix of form:\n",
    "    [1, tx, comb_mult(tx, 2), ..., comb_mult(tx, j)]\n",
    "    comb_mult(tx, 2) denotes all multiplicative combinations of the selected columns of tx.\n",
    "    If cols is not given, it returns the combinations of all columns of tx.\n",
    "    \"\"\"\n",
    "    poly = np.ones((tx.shape[0], 1))\n",
    "\n",
    "    for j in range(1, degree+1):\n",
    "        mult = build_mult_comb(tx, j, cols)\n",
    "        poly = np.column_stack((poly, mult))\n",
    "\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "data_path = \"../data/train.csv\"\n",
    "yb, input_data, ids = load_csv_data(data_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data to more useable data, by replacing NaN's by the column mean and creating a polynomial base expansion.\n",
    "\n",
    "By previous data analysis, important_cols are selected to be the few \"most influencial\" features. We use only those few weights in the polynomial base expansion for computational efficiency.\n",
    "\n",
    "| Index | Feature                   |\n",
    "|-------|---------------------------|\n",
    "|  0    | DER_mass_MMC              |\n",
    "| 1     | DER_mass_traverse_met_lep |\n",
    "| 2     | DER_mass_vis              |\n",
    "| 13    | PRI_tau_pt                |\n",
    "| 11    | DER_met_phi_centrality    |\n",
    "| 10    | DER_pt_ratio_lep_tau      |\n",
    "| 7     | DER_deltar_tau_lep        |\n",
    "| 19    | PRI_met                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = replace_nan_by_median(input_data, -999)\n",
    "\n",
    "important_cols = [0, 1, 2, 7, 10, 11, 13, 19]\n",
    "degree = 2 # With current implementation, higher than 5 is comp. infeasable, but it gives the best results!\n",
    "\n",
    "x_poly = build_advanced_poly(input_data, degree, important_cols)\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x_poly, yb, ratio=0.9, seed=123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.000010000, Training RMSE=0.807, Testing RMSE=0.804\n",
      "lambda=0.000022758, Training RMSE=0.807, Testing RMSE=0.804\n",
      "lambda=0.000051795, Training RMSE=0.807, Testing RMSE=0.804\n",
      "lambda=0.000117877, Training RMSE=0.807, Testing RMSE=0.805\n",
      "lambda=0.000268270, Training RMSE=0.807, Testing RMSE=0.805\n",
      "lambda=0.000610540, Training RMSE=0.808, Testing RMSE=0.807\n",
      "lambda=0.001389495, Training RMSE=0.810, Testing RMSE=0.809\n",
      "lambda=0.003162278, Training RMSE=0.812, Testing RMSE=0.811\n",
      "lambda=0.007196857, Training RMSE=0.815, Testing RMSE=0.815\n",
      "lambda=0.016378937, Training RMSE=0.819, Testing RMSE=0.819\n",
      "lambda=0.037275937, Training RMSE=0.823, Testing RMSE=0.824\n",
      "lambda=0.084834290, Training RMSE=0.827, Testing RMSE=0.828\n",
      "lambda=0.193069773, Training RMSE=0.829, Testing RMSE=0.830\n",
      "lambda=0.439397056, Training RMSE=0.831, Testing RMSE=0.832\n",
      "lambda=1.000000000, Training RMSE=0.832, Testing RMSE=0.833\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEaCAYAAAA7YdFPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e250828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plots import *\n",
    "\n",
    "# Experiment to find best lambda\n",
    "def ridge_regression_demo():\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 15)\n",
    "\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        weights, rmse = ridge_regression(y_train, x_train, lambda_)\n",
    "        rmse_tr.append(compute_rmse(y_train, x_train, weights))\n",
    "        rmse_te.append(compute_rmse(y_test, x_test, weights))\n",
    "        \n",
    "        print(\"lambda={l:.9f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "               l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "        \n",
    "    # Plot the obtained results\n",
    "    plot_train_test(rmse_tr, rmse_te, lambdas, 2)\n",
    "    \n",
    "ridge_regression_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Help function\"\"\"\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Help function\"\"\"\n",
    "def split_cross_data(y, x, k_indices, k):\n",
    "    k_test = k-1\n",
    "    test_ind = k_indices[k_test]\n",
    "    tr_ind = np.delete(k_indices, k_test, axis=0)\n",
    "    tr_ind = np.array(tr_ind).flatten()\n",
    "    x_te = x[test_ind]\n",
    "    y_te = y[test_ind]\n",
    "    x_tr = x[tr_ind]\n",
    "    y_tr = y[tr_ind]\n",
    "    return x_tr, y_tr, x_te, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    poly_base = np.ones((x.shape[0], 1))\n",
    "    for j in range(1, degree+1):\n",
    "        poly_base = np.column_stack((poly_base, np.power(x, j)))\n",
    "    poly_base = np.array(poly_base)\n",
    "    return poly_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Help function\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cross_validation_visualization(lambds, mse_tr, mse_te):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"cross_validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k, lambda_, k_indices, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    loss_tr = []\n",
    "    loss_te = []\n",
    "    important_cols = [0, 1, 2, 7, 10, 11, 13, 19]\n",
    "    degree = 2 # With current implementation, higher than 5 is comp. infeasable, but it gives the best results!\n",
    "    for k in range(k):\n",
    "        x_tr, y_tr, x_te, y_te = split_cross_data(y,x,k_indices,k)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # get k'th subgroup in test, others in train: TODO\n",
    "        # ***************************************************\n",
    "        poly_tr = build_advanced_poly(x_tr, degree, important_cols)\n",
    "        poly_te = build_advanced_poly(x_te, degree, important_cols)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # form data with polynomial degree: TODO\n",
    "        # ***************************************************\n",
    "        w, tr_rmse = ridge_regression(y_tr, poly_tr, lambda_)\n",
    "        \n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # ridge regression: TODO\n",
    "        # ***************************************************\n",
    "        te_rmse = compute_rmse(y_te, poly_te, w)\n",
    "        \n",
    "        loss_tr.append(tr_rmse)\n",
    "        loss_te.append(te_rmse)\n",
    "        \n",
    "    avg_tr = np.array(loss_tr).mean()\n",
    "    avg_te = np.array(loss_te).mean()\n",
    "    \n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # calculate the loss for train and test data: TODO\n",
    "        # ***************************************************\n",
    "        \n",
    "    return avg_tr, avg_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.763661906786544e-07, 1.4182529342883738)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1\n",
    "degree = 7\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-4, 0, 30)\n",
    "lambda_ = -3\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "cross_validation(yb, x_poly, k_fold, lambda_, k_indices, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12aed9668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from plots import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 7\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    lambda_ = 0.8\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    \n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # ridge regression with a given lambda\n",
    "        # ***************************************************\n",
    "        loss_tr, loss_te = cross_validation(yb, input_data, k_fold, lambda_, k_indices, degree)\n",
    "        \n",
    "        rmse_tr.append(loss_tr)\n",
    "        rmse_te.append(loss_te)\n",
    "        \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # cross validation: TODO\n",
    "    # ***************************************************    \n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    \n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in best lambda from above and get weights\n",
    "weights, rmse = ridge_regression(y_train, x_train, lambda_=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct: 192475 \n",
      "Total incorrect: 57525 \n",
      "Correct percentage: 76.99000000000001 %\n",
      "-----------------------------\n",
      "Train RMSE: 0.806750582846 , Test RMSE: 0.80393570686\n"
     ]
    }
   ],
   "source": [
    "# Predict labels with found weights and print some useful information about quality of fit\n",
    "y_pred = predict_labels(weights, x_poly)\n",
    "correctness(yb, y_pred)\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "rmse_train = compute_rmse(y_train, x_train, weights)\n",
    "rmse_test = compute_rmse(y_test, x_test, weights)\n",
    "print(\"Train RMSE:\", rmse_train, \", Test RMSE:\", rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and transform test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"../data/test.csv\"\n",
    "yb_test, input_data_test, ids_test = load_csv_data(test_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_test = replace_nan_by_median(input_data_test, -999)\n",
    "x_submit_poly = build_advanced_poly(input_data_test, degree, important_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = predict_labels(weights, x_submit_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions of test data in csv file, ready for the upload on kaggle\n",
    "create_csv_submission(ids_test, y_test_pred, \"test_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
