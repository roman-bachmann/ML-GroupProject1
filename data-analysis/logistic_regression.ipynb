{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import proj1_helpers\n",
    "\n",
    "from mlcomp.config import DATA_PATH\n",
    "from mlcomp.data import load_csv_data, create_csv_submission\n",
    "from mlcomp.helpers import split_data, compute_rmse\n",
    "from mlcomp.performance import correctness\n",
    "from mlcomp.feature_eng import build_advanced_poly\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data(TRAIN_PATH, sub_sample=False)\n",
    "yb = yb.reshape((yb.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    loss = 0\n",
    "    for n in range(N):\n",
    "        xnw = np.dot(tx[n], w)\n",
    "        loss += np.log(1 + np.exp(xnw)) - y[n]*xnw\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return np.dot(tx.T, sigmoid(np.dot(tx, w)) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_diff(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def logistic_regression_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    S = sigmoid_diff(np.dot(tx, w))\n",
    "    return np.dot(tx.T, S * tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newton_step(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    D = tx.shape[1] if tx.shape[1:] else 1\n",
    "\n",
    "    loss = logistic_regression_loss(y, tx, w)\n",
    "    grad = logistic_regression_gradient(y, tx, w)\n",
    "    hess = logistic_regression_hessian(y, tx, w)\n",
    "    \n",
    "    hess_inv = np.linalg.solve(hess, np.identity(D))\n",
    "    w = w - gamma * np.dot(hess_inv, grad)\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 0.0001 # 1e-8\n",
    "    lambda_ = 0.1\n",
    "    losses = []\n",
    "    gamma = 0.01\n",
    "    \n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        if (len(losses) > 1 and losses[-1] < 1):\n",
    "            gamma = 0.00000001\n",
    "        elif (len(losses) > 1 and losses[-1] < 15):\n",
    "            gamma = 0.0000001\n",
    "        elif (len(losses) > 1 and losses[-1] < 200):\n",
    "            gamma = 0.00001\n",
    "        elif (len(losses) > 1 and losses[-1] < 5000):\n",
    "            gamma = 0.0001\n",
    "        elif (len(losses) > 1 and losses[-1] < 20000):\n",
    "            gamma = 0.001\n",
    "            \n",
    "        loss, w = newton_step(y, tx, w, gamma)\n",
    "        # log info\n",
    "        print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_step(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = logistic_regression_loss(y, tx, w)\n",
    "    grad = logistic_regression_gradient(y, tx, w)\n",
    "    w = w - gamma * grad\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.01\n",
    "    losses = []\n",
    "    \n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = logistic_regression_gradient_descent_step(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important_cols = [0, 1, 2, 7, 10, 11, 13, 19]\n",
    "important_cols = [0, 1, 2, 10, 11, 13]\n",
    "degree = 3\n",
    "\n",
    "tx = build_advanced_poly(input_data, degree, important_cols)\n",
    "#tx = np.c_[np.ones((input_data.shape[0], 1)), input_data]\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(tx, yb, ratio=0.9, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=[ 155958.11562618]\n",
      "Current iteration=1, the loss=[ 147764.91582331]\n",
      "Current iteration=2, the loss=[ 139728.40216235]\n",
      "Current iteration=3, the loss=[ 131833.3326417]\n",
      "Current iteration=4, the loss=[ 124065.45170514]\n",
      "Current iteration=5, the loss=[ 116411.29955439]\n",
      "Current iteration=6, the loss=[ 108858.0447108]\n",
      "Current iteration=7, the loss=[ 101393.34246607]\n",
      "Current iteration=8, the loss=[ 94005.23708427]\n",
      "Current iteration=9, the loss=[ 86682.1489092]\n",
      "Current iteration=10, the loss=[ 79412.98473542]\n",
      "Current iteration=11, the loss=[ 72187.24477356]\n",
      "Current iteration=12, the loss=[ 64994.78643889]\n",
      "Current iteration=13, the loss=[ 57825.36075265]\n",
      "Current iteration=14, the loss=[ 50668.42096854]\n",
      "Current iteration=15, the loss=[ 43513.11888416]\n",
      "Current iteration=16, the loss=[ 36348.2143641]\n",
      "Current iteration=17, the loss=[ 29161.88546131]\n",
      "Current iteration=18, the loss=[ 21941.52324619]\n",
      "Current iteration=19, the loss=[ 14673.53836265]\n",
      "Current iteration=20, the loss=[ 7343.15632827]\n",
      "Current iteration=21, the loss=[ 6598.94651223]\n",
      "Current iteration=22, the loss=[ 5853.82824666]\n",
      "Current iteration=23, the loss=[ 5107.78254029]\n",
      "Current iteration=24, the loss=[ 4360.79018097]\n",
      "Current iteration=25, the loss=[ 3612.83173088]\n",
      "Current iteration=26, the loss=[ 3537.9036318]\n",
      "Current iteration=27, the loss=[ 3462.96553482]\n",
      "Current iteration=28, the loss=[ 3388.01741991]\n",
      "Current iteration=29, the loss=[ 3313.05926701]\n",
      "Current iteration=30, the loss=[ 3238.09105604]\n",
      "Current iteration=31, the loss=[ 3163.1127669]\n",
      "Current iteration=32, the loss=[ 3088.12437946]\n",
      "Current iteration=33, the loss=[ 3013.12587356]\n",
      "Current iteration=34, the loss=[ 2938.11722903]\n",
      "Current iteration=35, the loss=[ 2863.09842567]\n",
      "Current iteration=36, the loss=[ 2788.06944325]\n",
      "Current iteration=37, the loss=[ 2713.03026151]\n",
      "Current iteration=38, the loss=[ 2637.98086018]\n",
      "Current iteration=39, the loss=[ 2562.92121897]\n",
      "Current iteration=40, the loss=[ 2487.85131753]\n",
      "Current iteration=41, the loss=[ 2412.77113553]\n",
      "Current iteration=42, the loss=[ 2337.68065259]\n",
      "Current iteration=43, the loss=[ 2262.57984831]\n",
      "Current iteration=44, the loss=[ 2187.46870226]\n",
      "Current iteration=45, the loss=[ 2112.34719399]\n",
      "Current iteration=46, the loss=[ 2037.21530303]\n",
      "Current iteration=47, the loss=[ 1962.07300887]\n",
      "Current iteration=48, the loss=[ 1886.92029099]\n",
      "Current iteration=49, the loss=[ 1811.75712883]\n",
      "Current iteration=50, the loss=[ 1736.58350182]\n",
      "Current iteration=51, the loss=[ 1661.39938936]\n",
      "Current iteration=52, the loss=[ 1586.20477082]\n",
      "Current iteration=53, the loss=[ 1510.99962555]\n",
      "Current iteration=54, the loss=[ 1435.78393285]\n",
      "Current iteration=55, the loss=[ 1360.55767205]\n",
      "Current iteration=56, the loss=[ 1285.32082239]\n",
      "Current iteration=57, the loss=[ 1210.07336312]\n",
      "Current iteration=58, the loss=[ 1134.81527347]\n",
      "Current iteration=59, the loss=[ 1059.54653262]\n",
      "Current iteration=60, the loss=[ 984.26711974]\n",
      "Current iteration=61, the loss=[ 908.97701398]\n",
      "Current iteration=62, the loss=[ 833.67619443]\n",
      "Current iteration=63, the loss=[ 758.3646402]\n",
      "Current iteration=64, the loss=[ 683.04233034]\n",
      "Current iteration=65, the loss=[ 607.70924389]\n",
      "Current iteration=66, the loss=[ 532.36535986]\n",
      "Current iteration=67, the loss=[ 457.01065722]\n",
      "Current iteration=68, the loss=[ 381.64511494]\n",
      "Current iteration=69, the loss=[ 306.26871194]\n",
      "Current iteration=70, the loss=[ 230.88142714]\n",
      "Current iteration=71, the loss=[ 155.48323939]\n",
      "Current iteration=72, the loss=[ 80.07412756]\n",
      "Current iteration=73, the loss=[ 72.53178249]\n",
      "Current iteration=74, the loss=[ 64.98932781]\n",
      "Current iteration=75, the loss=[ 57.44676351]\n",
      "Current iteration=76, the loss=[ 49.90408956]\n",
      "Current iteration=77, the loss=[ 42.36130594]\n",
      "Current iteration=78, the loss=[ 34.81841264]\n",
      "Current iteration=79, the loss=[ 27.27540962]\n",
      "Current iteration=80, the loss=[ 19.73229688]\n",
      "Current iteration=81, the loss=[ 12.18907438]\n",
      "Current iteration=82, the loss=[ 4.64574211]\n",
      "Current iteration=83, the loss=[ 4.57030731]\n",
      "Current iteration=84, the loss=[ 4.49487251]\n",
      "Current iteration=85, the loss=[ 4.41943769]\n",
      "Current iteration=86, the loss=[ 4.34400287]\n",
      "Current iteration=87, the loss=[ 4.26856803]\n",
      "Current iteration=88, the loss=[ 4.19313318]\n",
      "Current iteration=89, the loss=[ 4.11769832]\n",
      "Current iteration=90, the loss=[ 4.04226345]\n",
      "Current iteration=91, the loss=[ 3.96682857]\n",
      "Current iteration=92, the loss=[ 3.89139367]\n",
      "Current iteration=93, the loss=[ 3.81595877]\n",
      "Current iteration=94, the loss=[ 3.74052385]\n",
      "Current iteration=95, the loss=[ 3.66508893]\n",
      "Current iteration=96, the loss=[ 3.58965399]\n",
      "Current iteration=97, the loss=[ 3.51421904]\n",
      "Current iteration=98, the loss=[ 3.43878409]\n",
      "Current iteration=99, the loss=[ 3.36334912]\n"
     ]
    }
   ],
   "source": [
    "weights = logistic_regression(y_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct: 172182 \n",
      "Total incorrect: 77818 \n",
      "Correct percentage: 68.8728 %\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import predict_labels\n",
    "\n",
    "# Predict labels with found weights and print some useful information about quality of fit\n",
    "y_pred = predict_labels(weights, tx)\n",
    "correctness(yb, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids, y_pred, \"test_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
