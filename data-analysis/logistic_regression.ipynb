{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import proj1_helpers\n",
    "\n",
    "from mlcomp.config import DATA_PATH\n",
    "from mlcomp.data import load_csv_data, create_csv_submission\n",
    "from mlcomp.helpers import split_data, compute_rmse\n",
    "from mlcomp.performance import correctness\n",
    "from mlcomp.feature_eng import build_advanced_poly\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data(TRAIN_PATH, sub_sample=False)\n",
    "yb_orig = yb.copy()\n",
    "yb_orig = yb_orig.reshape((yb_orig.shape[0], 1))\n",
    "yb[yb == -1] = 0\n",
    "yb = yb.reshape((yb.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       ..., \n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    loss = 0\n",
    "    for n in range(N):\n",
    "        xnw = np.dot(tx[n], w)\n",
    "        loss += np.log(1 + np.exp(xnw)) - y[n]*xnw\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return np.dot(tx.T, sigmoid(np.dot(tx, w)) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_diff(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def logistic_regression_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    S = sigmoid_diff(np.dot(tx, w))\n",
    "    return np.dot(tx.T, S * tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_step(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    D = tx.shape[1] if tx.shape[1:] else 1\n",
    "\n",
    "    loss = logistic_regression_loss(y, tx, w)\n",
    "    grad = logistic_regression_gradient(y, tx, w)\n",
    "    hess = logistic_regression_hessian(y, tx, w)\n",
    "    \n",
    "    hess_inv = np.linalg.solve(hess, np.identity(D))\n",
    "    w = w - gamma * np.dot(hess_inv, grad)\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 0.1 # 1e-8\n",
    "    lambda_ = 0.1\n",
    "    losses = []\n",
    "    gamma = 0.3\n",
    "    \n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        \"\"\"\n",
    "        if (len(losses) > 1 and losses[-1] < 1):\n",
    "            gamma = 0.00000001\n",
    "        elif (len(losses) > 1 and losses[-1] < 15):\n",
    "            gamma = 0.0000001\n",
    "        elif (len(losses) > 1 and losses[-1] < 200):\n",
    "            gamma = 0.00001\n",
    "        elif (len(losses) > 1 and losses[-1] < 5000):\n",
    "            gamma = 0.0001\n",
    "        elif (len(losses) > 1 and losses[-1] < 20000):\n",
    "            gamma = 0.001\n",
    "        \"\"\"\n",
    "            \n",
    "        loss, w = newton_step(y, tx, w, gamma)\n",
    "        # log info\n",
    "        print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_step(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = logistic_regression_loss(y, tx, w)\n",
    "    grad = logistic_regression_gradient(y, tx, w)\n",
    "    w = w - gamma * grad\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent(y, tx):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.01\n",
    "    losses = []\n",
    "    \n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = logistic_regression_gradient_descent_step(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_by_median(tx, nan_value):\n",
    "    \"\"\"Replaces values with a specified nan_value by the column median.\"\"\"\n",
    "    tx[tx == nan_value] = np.nan\n",
    "    col_median = np.nanmedian(tx, axis=0)\n",
    "    return np.where(np.isnan(tx), col_median, tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.27272727e-01,   5.38461538e-01,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   3.33444481e-04],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   1.00000000e+00]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, -5, 6], [-7, 8, 9000]])\n",
    "x = (x - x.min(0)) / x.ptp(0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important_cols = [0, 1, 2, 7, 10, 11, 13, 19]\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "tx = replace_nan_by_median(input_data, -999)\n",
    "important_cols = [0, 1, 2, 7, 10, 11, 13, 19]\n",
    "degree = 4\n",
    "poly = PolynomialFeatures(degree)\n",
    "x_poly = poly.fit_transform(tx[:, important_cols])\n",
    "x_poly = normalize(x_poly, axis=0, norm='max')\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x_poly, yb, ratio=0.9, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_poly.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=[ 155958.11562618]\n",
      "Current iteration=1, the loss=[ 130022.17113303]\n",
      "Current iteration=2, the loss=[ 115757.16928569]\n",
      "Current iteration=3, the loss=[ 106910.87901842]\n",
      "Current iteration=4, the loss=[ 101148.44961195]\n",
      "Current iteration=5, the loss=[ 97314.83565042]\n",
      "Current iteration=6, the loss=[ 94741.07652506]\n",
      "Current iteration=7, the loss=[ 93004.16100331]\n",
      "Current iteration=8, the loss=[ 91840.99929786]\n",
      "Current iteration=9, the loss=[ 91070.55479957]\n",
      "Current iteration=10, the loss=[ 90564.49776047]\n",
      "Current iteration=11, the loss=[ 90233.18911906]\n",
      "Current iteration=12, the loss=[ 90019.78209352]\n",
      "Current iteration=13, the loss=[ 89881.28157526]\n",
      "Current iteration=14, the loss=[ 89792.96301521]\n",
      "Current iteration=15, the loss=[ 89735.78461935]\n",
      "Current iteration=16, the loss=[ 89700.10910937]\n",
      "Current iteration=17, the loss=[ 89677.93835444]\n",
      "Current iteration=18, the loss=[ 89663.78904491]\n",
      "Current iteration=19, the loss=[ 89652.4453274]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=20, the loss=[ 89645.6816281]\n",
      "Current iteration=21, the loss=[ 89641.64638013]\n",
      "Current iteration=22, the loss=[ 89639.46429906]\n",
      "Current iteration=23, the loss=[ 89638.14616562]\n",
      "Current iteration=24, the loss=[ 89634.37263575]\n",
      "Current iteration=25, the loss=[ 89631.74950051]\n",
      "Current iteration=26, the loss=[ 89629.84712511]\n",
      "Current iteration=27, the loss=[ 89628.54992638]\n",
      "Current iteration=28, the loss=[ 89627.68590933]\n",
      "Current iteration=29, the loss=[ 89627.21618255]\n",
      "Current iteration=30, the loss=[ 89626.95842696]\n",
      "Current iteration=31, the loss=[ 89626.49345555]\n",
      "Current iteration=32, the loss=[ 89626.13343175]\n",
      "Current iteration=33, the loss=[ 89625.94493734]\n",
      "Current iteration=34, the loss=[ 89625.84748253]\n"
     ]
    }
   ],
   "source": [
    "weights = logistic_regression(y_train, x_train)\n",
    "#weights = logistic_regression_gradient_descent(y_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct: 205391 \n",
      "Total incorrect: 44609 \n",
      "Correct percentage: 82.15639999999999 %\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import predict_labels\n",
    "\n",
    "# Predict labels with found weights and print some useful information about quality of fit\n",
    "y_pred = predict_labels(weights, x_poly)\n",
    "correctness(yb_orig, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"../data/test.csv\"\n",
    "yb_test, input_data_test, ids_test = load_csv_data(test_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_test = replace_nan_by_median(input_data_test, -999)\n",
    "important_cols = [0, 1, 2, 7, 10, 11, 13, 19]\n",
    "degree = 4\n",
    "poly_test = PolynomialFeatures(degree)\n",
    "x_poly_test = poly_test.fit_transform(tx_test[:, important_cols])\n",
    "x_poly_test = normalize(x_poly_test, axis=0, norm='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = predict_labels(weights, x_poly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_test_pred, \"test_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
