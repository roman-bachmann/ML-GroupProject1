{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Project 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlcomp.data import load_csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import proj1_helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations of ML Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data:\n",
    "\n",
    "Testing data for helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([1, 2, 3, 4])\n",
    "tx = np.array([[1, 2, 3], \n",
    "               [4, 5, 6], \n",
    "               [7, 8, 9], \n",
    "               [10, 11, 12]])\n",
    "w = np.array([0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss_mse(y, tx, w):\n",
    "    \"\"\"Calculate the MSE loss.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx, w)\n",
    "    return np.dot(e.T,e) / (2 * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss_mae(y, tx, w):\n",
    "    \"\"\"Calculate the MAE loss.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx, w)\n",
    "    return np.sum(np.absolute(e)) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rmse(y, tx, w):\n",
    "    \"\"\"Computes the Root Mean Square Error\"\"\"\n",
    "    mse = compute_loss_mse(y, tx, w)\n",
    "    return np.sqrt(2 * mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient_mse(y, tx, w):\n",
    "    \"\"\"Compute the MSE gradient.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx, w)\n",
    "    return (-1/N) * np.dot(np.transpose(tx), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_stochastic_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a stochastic subgradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx, w)\n",
    "    abs_e_subgrad = [np.sign(en) for en in e] # Sign chosen for subgradient of absolute value function\n",
    "    return (-1/N) * np.dot(np.transpose(tx), abs_e_subgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    np.random.seed(seed) # set seed\n",
    "    permuted_idxs = np.random.permutation(x.shape[0])\n",
    "    train_size = int(ratio * x.shape[0])\n",
    "    train_idxs, test_idxs = permuted_idxs[:train_size], permuted_idxs[train_size:]\n",
    "    \n",
    "    return x[train_idxs], x[test_idxs], y[train_idxs], y[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_indices(k_indices, k):\n",
    "    train_indices = np.array([])\n",
    "    for i in range(k_indices.shape[0]):\n",
    "        if (i != k-1):\n",
    "            train_indices = np.hstack((train_indices, k_indices[i]))\n",
    "    return train_indices.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to implement for project 1 submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Linear regression using gradient descent\"\"\"\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm using MSE.\"\"\"\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient_mse(y, tx, w)\n",
    "        loss = compute_loss_mse(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "    rmse = compute_rmse(y, tx, w)\n",
    "\n",
    "    return w, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Linear regression using stochastic gradient descent\"\"\"\n",
    "def stochastic_subgradient_descent_mae(y, tx, initial_w, max_iters, gamma, batch_size):\n",
    "    \"\"\"Stochastic subgradient descent algorithm using MAE.\"\"\"\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            g = compute_stochastic_subgradient_mae(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * g\n",
    "        loss = compute_loss_mae(y, tx, w)\n",
    "        print(\"Stochastic Subgradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculates the explicit least squares solution.\n",
    "    Returns rmse, optimal weights\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    D = tx.shape[1]\n",
    "    rank_tx = np.linalg.matrix_rank(tx)\n",
    "    \n",
    "    # Check if tx is invertible. If so, find explicit solution\n",
    "    # using real inverses.\n",
    "    # If not, find explicit solution using pseudoinverses.\n",
    "    if (rank_tx == max(tx.shape[0], tx.shape[1])):\n",
    "        gramian_inv = np.linalg.solve(np.dot(tx.T, tx), np.identity(D))\n",
    "        w = np.dot(gramian_inv, np.dot(tx.T, y))\n",
    "    else:\n",
    "        U, s, V_T = np.linalg.svd(tx)\n",
    "        S_inv_T = np.zeros((D, N))\n",
    "        S_inv_T[:len(s), :len(s)] = np.diag(1/s)\n",
    "        w = np.dot(V_T.T, np.dot(S_inv_T, np.dot(U.T, y)))\n",
    "    \n",
    "    rmse = compute_rmse(y, tx, w)\n",
    "    \n",
    "    return w, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"Ridge regression using normal equations\"\"\"\n",
    "    \n",
    "    #if (lambda_ == 0):\n",
    "    #    return least_squares(y, tx)\n",
    "    \n",
    "    N = tx.shape[0]\n",
    "    D = tx.shape[1]\n",
    "    \n",
    "    inv_inner = np.dot(tx.T, tx) + 2 * N * lambda_ * np.identity(D)\n",
    "    inv = np.linalg.solve(inv_inner, np.identity(D))\n",
    "    w = np.dot(inv, np.dot(tx.T, y))\n",
    "    \n",
    "    rmse = compute_rmse(y, tx, w)\n",
    "    \n",
    "    return w, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Logistic regression using gradient descent or SGD\"\"\"\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Regularized logistic regression using gradient descent or SGD\"\"\"\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higgs Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental functions\n",
    "\n",
    "All of these functions can be used experimentally to, in one way or the other, improve the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correctness(yb, y_pred):\n",
    "    \"\"\"Takes inputs known y and predicted y and prints the ratio of correct predictions vs incorrect ones.\"\"\"\n",
    "    correct = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if (y_pred[i] == yb[i]):\n",
    "            correct += 1\n",
    "        \n",
    "    incorrect = len(y_pred) - correct\n",
    "    perc = correct / len(y_pred) * 100\n",
    "    print(\"Total correct:\", correct, \"\\nTotal incorrect:\", incorrect, \"\\nCorrect percentage:\", perc, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_nan_by_mean(tx, nan_value):\n",
    "    \"\"\"Replaces values with a specified nan_value by the column mean.\"\"\"\n",
    "    tx[tx == nan_value] = np.nan\n",
    "    col_mean = np.nanmean(tx, axis=0)\n",
    "    return np.where(np.isnan(tx), col_mean, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_nan_by_median(tx, nan_value):\n",
    "    \"\"\"Replaces values with a specified nan_value by the column median.\"\"\"\n",
    "    tx[tx == nan_value] = np.nan\n",
    "    col_median = np.nanmedian(tx, axis=0)\n",
    "    return np.where(np.isnan(tx), col_median, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_nan_rows(tx, nan_value):\n",
    "    \"\"\"Drop all rows that contain a nan equaling the specified nan_value.\"\"\"\n",
    "    tx[tx == nan_value] = np.nan\n",
    "    return tx[~np.isnan(tx).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_poly(tx, degree):\n",
    "    \"\"\"\n",
    "    Builds simple polynomial basis function for input data matrix tx, for j=0 up to j=degree,\n",
    "    where the result will be a matrix of form [1, tx, tx^2, ..., tx^j].\n",
    "    tx^j denotes that for each x_i,k in tx, the result will be (x_i,k)^j\n",
    "    \"\"\"\n",
    "    poly = np.ones((tx.shape[0], 1))\n",
    "\n",
    "    for j in range(1, degree+1):\n",
    "        poly = np.column_stack((poly, np.power(tx, j)))\n",
    "\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# TODO: Optimize. Very slow!\n",
    "def build_mult_comb(tx, deg, cols=[]):\n",
    "    \"\"\"\n",
    "    Returns all multiplicative combinations of the specified columns for degree deg.\n",
    "    For len(col) = D', there are (D' choose deg) combinations of columns that get\n",
    "    returned as a matrix.\n",
    "    If cols is not given, it returns the combinations of all columns of tx.\n",
    "    \"\"\"\n",
    "    N = tx.shape[0]\n",
    "    if (cols == []):\n",
    "        comb_iter = itertools.combinations_with_replacement(range(tx.shape[1]), deg)\n",
    "    else:\n",
    "        comb_iter = itertools.combinations_with_replacement(cols, deg)\n",
    "    mult = []\n",
    "    for comb in comb_iter:\n",
    "        mult_col = np.ones(N)\n",
    "        for idx in comb:\n",
    "            tx_col = tx[:,idx]\n",
    "            mult_col = np.multiply(mult_col, tx_col)\n",
    "        mult.append(mult_col.tolist())\n",
    "    return np.array(mult).T\n",
    "\n",
    "def build_advanced_poly(tx, degree, cols=[]):\n",
    "    \"\"\"\n",
    "    Builds full polynomial basis function for input data matrix tx, for j=0 up to j=degree,\n",
    "    where the result will be a matrix of form:\n",
    "    [1, tx, comb_mult(tx, 2), ..., comb_mult(tx, j)]\n",
    "    comb_mult(tx, 2) denotes all multiplicative combinations of the selected columns of tx.\n",
    "    If cols is not given, it returns the combinations of all columns of tx.\n",
    "    \"\"\"\n",
    "    poly = np.ones((tx.shape[0], 1))\n",
    "\n",
    "    for j in range(1, degree+1):\n",
    "        mult = build_mult_comb(tx, j, cols)\n",
    "        poly = np.column_stack((poly, mult))\n",
    "\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "data_path = \"../data/train.csv\"\n",
    "yb, input_data, ids = load_csv_data(data_path, sub_sample=False)\n",
    "yb = yb.reshape((yb.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 1) (250000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(yb.shape, input_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data to more useable data, by replacing NaN's by the column mean and creating a polynomial base expansion.\n",
    "\n",
    "By previous data analysis, important_cols are selected to be the few \"most influencial\" features. We use only those few weights in the polynomial base expansion for computational efficiency.\n",
    "\n",
    "| Index | Feature                   |\n",
    "|-------|---------------------------|\n",
    "|  0    | DER_mass_MMC              |\n",
    "| 1     | DER_mass_traverse_met_lep |\n",
    "| 2     | DER_mass_vis              |\n",
    "| 13    | PRI_tau_pt                |\n",
    "| 11    | DER_met_phi_centrality    |\n",
    "| 10    | DER_pt_ratio_lep_tau      |\n",
    "| 7     | DER_deltar_tau_lep        |\n",
    "| 19    | PRI_met                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = replace_nan_by_median(input_data, -999)\n",
    "\n",
    "important_cols = [0, 1, 2, 7, 10, 11, 13, 19]\n",
    "#important_cols = [0, 1, 2, 10, 11, 13]\n",
    "degree = 2 # With current implementation, higher than 5 is comp. infeasable, but it gives the best results!\n",
    "\n",
    "x_poly = build_advanced_poly(input_data, degree, important_cols)\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x_poly, yb, ratio=0.9, seed=123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_w = np.zeros((x_train.shape[1], 1))\n",
    "max_iters = 100\n",
    "gamma = 1\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Subgradient Descent(0/99): loss=233339751.11872205, w0=[-1.], w1=[-77.112]\n",
      "Stochastic Subgradient Descent(1/99): loss=72292625.66239856, w0=[ 0.], w1=[ 21.414]\n",
      "Stochastic Subgradient Descent(2/99): loss=271285380.54342985, w0=[-1.], w1=[-73.125]\n",
      "Stochastic Subgradient Descent(3/99): loss=91942031.41665281, w0=[ 0.], w1=[ 25.157]\n",
      "Stochastic Subgradient Descent(4/99): loss=669453570.8104852, w0=[-1.], w1=[-123.896]\n",
      "Stochastic Subgradient Descent(5/99): loss=151155430.155957, w0=[ 0.], w1=[-11.49]\n",
      "Stochastic Subgradient Descent(6/99): loss=493021424.5928208, w0=[ 1.], w1=[ 119.291]\n",
      "Stochastic Subgradient Descent(7/99): loss=66687581.32247934, w0=[ 0.], w1=[ 12.542]\n",
      "Stochastic Subgradient Descent(8/99): loss=560973129.1062058, w0=[-1.], w1=[-95.854]\n",
      "Stochastic Subgradient Descent(9/99): loss=88444150.82411285, w0=[ 0.], w1=[ 20.846]\n",
      "Stochastic Subgradient Descent(10/99): loss=979620421.6364462, w0=[ 1.], w1=[ 184.794]\n",
      "Stochastic Subgradient Descent(11/99): loss=380819345.6720501, w0=[ 0.], w1=[ 50.767]\n",
      "Stochastic Subgradient Descent(12/99): loss=835434370.0404588, w0=[ 1.], w1=[ 142.47]\n",
      "Stochastic Subgradient Descent(13/99): loss=433092178.9206668, w0=[ 0.], w1=[ 32.721]\n",
      "Stochastic Subgradient Descent(14/99): loss=178340892.777708, w0=[-1.], w1=[-43.188]\n",
      "Stochastic Subgradient Descent(15/99): loss=376437585.4898, w0=[-2.], w1=[-154.892]\n",
      "Stochastic Subgradient Descent(16/99): loss=1529305073.8981454, w0=[-1.], w1=[ 82.622]\n",
      "Stochastic Subgradient Descent(17/99): loss=1250807963.0324829, w0=[-2.], w1=[-8.062]\n",
      "Stochastic Subgradient Descent(18/99): loss=827334454.2744235, w0=[-3.], w1=[-120.468]\n",
      "Stochastic Subgradient Descent(19/99): loss=556471680.0866191, w0=[-4.], w1=[-198.926]\n",
      "Stochastic Subgradient Descent(20/99): loss=418070682.80743027, w0=[-5.], w1=[-247.56]\n",
      "Stochastic Subgradient Descent(21/99): loss=156708295.14057595, w0=[-6.], w1=[-359.966]\n",
      "Stochastic Subgradient Descent(22/99): loss=2359254064.4061832, w0=[-7.], w1=[-627.071]\n",
      "Stochastic Subgradient Descent(23/99): loss=1685488103.045097, w0=[-6.], w1=[-514.665]\n",
      "Stochastic Subgradient Descent(24/99): loss=1196946127.9381955, w0=[-5.], w1=[-391.1]\n",
      "Stochastic Subgradient Descent(25/99): loss=593805468.3112165, w0=[-4.], w1=[-257.123]\n",
      "Stochastic Subgradient Descent(26/99): loss=289253486.2305284, w0=[-3.], w1=[-97.067]\n",
      "Stochastic Subgradient Descent(27/99): loss=219902340.20287666, w0=[-4.], w1=[-202.335]\n",
      "Stochastic Subgradient Descent(28/99): loss=690611644.865324, w0=[-5.], w1=[-320.754]\n",
      "Stochastic Subgradient Descent(29/99): loss=145147206.85307533, w0=[-4.], w1=[-192.343]\n",
      "Stochastic Subgradient Descent(30/99): loss=297679922.1529236, w0=[-3.], w1=[-79.793]\n",
      "Stochastic Subgradient Descent(31/99): loss=828300377.5080358, w0=[-4.], w1=[-268.468]\n",
      "Stochastic Subgradient Descent(32/99): loss=223949654.95095518, w0=[-3.], w1=[-156.062]\n",
      "Stochastic Subgradient Descent(33/99): loss=287644739.77468437, w0=[-4.], w1=[-195.807]\n",
      "Stochastic Subgradient Descent(34/99): loss=224373342.73399308, w0=[-3.], w1=[-103.503]\n",
      "Stochastic Subgradient Descent(35/99): loss=448803355.7872111, w0=[-4.], w1=[-220.118]\n",
      "Stochastic Subgradient Descent(36/99): loss=276216787.3296034, w0=[-3.], w1=[-107.712]\n",
      "Stochastic Subgradient Descent(37/99): loss=476119978.01672006, w0=[-4.], w1=[-241.868]\n",
      "Stochastic Subgradient Descent(38/99): loss=330776402.4449116, w0=[-3.], w1=[-103.736]\n",
      "Stochastic Subgradient Descent(39/99): loss=392353421.4816246, w0=[-4.], w1=[-222.556]\n",
      "Stochastic Subgradient Descent(40/99): loss=300782026.3234287, w0=[-3.], w1=[-89.17]\n",
      "Stochastic Subgradient Descent(41/99): loss=156147661.78389, w0=[-4.], w1=[-175.279]\n",
      "Stochastic Subgradient Descent(42/99): loss=1136434401.7424545, w0=[-5.], w1=[-326.41]\n",
      "Stochastic Subgradient Descent(43/99): loss=454504505.654496, w0=[-4.], w1=[-193.264]\n",
      "Stochastic Subgradient Descent(44/99): loss=151106833.60928375, w0=[-3.], w1=[-80.858]\n",
      "Stochastic Subgradient Descent(45/99): loss=380415258.78424835, w0=[-4.], w1=[-173.608]\n",
      "Stochastic Subgradient Descent(46/99): loss=133696578.79046954, w0=[-3.], w1=[-63.805]\n",
      "Stochastic Subgradient Descent(47/99): loss=719594898.0900891, w0=[-4.], w1=[-176.211]\n",
      "Stochastic Subgradient Descent(48/99): loss=133563092.358412, w0=[-3.], w1=[-54.256]\n",
      "Stochastic Subgradient Descent(49/99): loss=224674660.94321632, w0=[-2.], w1=[ 20.542]\n",
      "Stochastic Subgradient Descent(50/99): loss=520714584.41801804, w0=[-1.], w1=[ 94.42]\n",
      "Stochastic Subgradient Descent(51/99): loss=238102767.88853723, w0=[-2.], w1=[ 4.346]\n",
      "Stochastic Subgradient Descent(52/99): loss=438472549.0662959, w0=[-3.], w1=[-108.06]\n",
      "Stochastic Subgradient Descent(53/99): loss=222442441.3331545, w0=[-2.], w1=[-25.992]\n",
      "Stochastic Subgradient Descent(54/99): loss=478000906.80772126, w0=[-1.], w1=[ 86.414]\n",
      "Stochastic Subgradient Descent(55/99): loss=171785793.06700015, w0=[-2.], w1=[-19.642]\n",
      "Stochastic Subgradient Descent(56/99): loss=366909837.391759, w0=[-3.], w1=[-113.173]\n",
      "Stochastic Subgradient Descent(57/99): loss=266551078.890166, w0=[-2.], w1=[-7.78]\n",
      "Stochastic Subgradient Descent(58/99): loss=183008820.5910287, w0=[-3.], w1=[-107.34]\n",
      "Stochastic Subgradient Descent(59/99): loss=286428729.2399255, w0=[-2.], w1=[-16.11]\n",
      "Stochastic Subgradient Descent(60/99): loss=289669492.9596613, w0=[-3.], w1=[-137.157]\n",
      "Stochastic Subgradient Descent(61/99): loss=365311056.11877733, w0=[-2.], w1=[-11.216]\n",
      "Stochastic Subgradient Descent(62/99): loss=217179130.71046117, w0=[-3.], w1=[-126.197]\n",
      "Stochastic Subgradient Descent(63/99): loss=174156306.31829396, w0=[-2.], w1=[-64.073]\n",
      "Stochastic Subgradient Descent(64/99): loss=750975311.9504013, w0=[-1.], w1=[ 79.221]\n",
      "Stochastic Subgradient Descent(65/99): loss=253379983.2948777, w0=[-2.], w1=[-33.185]\n",
      "Stochastic Subgradient Descent(66/99): loss=213513047.8162861, w0=[-3.], w1=[-109.986]\n",
      "Stochastic Subgradient Descent(67/99): loss=879919795.6005421, w0=[-4.], w1=[-252.164]\n",
      "Stochastic Subgradient Descent(68/99): loss=492264971.02393526, w0=[-3.], w1=[-142.995]\n",
      "Stochastic Subgradient Descent(69/99): loss=217654224.02147642, w0=[-2.], w1=[-48.356]\n",
      "Stochastic Subgradient Descent(70/99): loss=1014941096.6976161, w0=[-3.], w1=[-204.574]\n",
      "Stochastic Subgradient Descent(71/99): loss=179189979.01777282, w0=[-2.], w1=[-33.293]\n",
      "Stochastic Subgradient Descent(72/99): loss=431117100.24528915, w0=[-3.], w1=[-143.804]\n",
      "Stochastic Subgradient Descent(73/99): loss=171828112.77850768, w0=[-2.], w1=[-37.1]\n",
      "Stochastic Subgradient Descent(74/99): loss=952148579.7653112, w0=[-3.], w1=[-193.153]\n",
      "Stochastic Subgradient Descent(75/99): loss=570847092.0851797, w0=[-2.], w1=[-86.057]\n",
      "Stochastic Subgradient Descent(76/99): loss=334901451.2221404, w0=[-1.], w1=[-5.47]\n",
      "Stochastic Subgradient Descent(77/99): loss=211402808.47979042, w0=[ 0.], w1=[ 105.263]\n",
      "Stochastic Subgradient Descent(78/99): loss=444120279.81395954, w0=[-1.], w1=[-23.094]\n",
      "Stochastic Subgradient Descent(79/99): loss=4032567461.296405, w0=[ 0.], w1=[ 308.86]\n",
      "Stochastic Subgradient Descent(80/99): loss=3705820536.326176, w0=[-1.], w1=[ 235.482]\n",
      "Stochastic Subgradient Descent(81/99): loss=2964185542.0248923, w0=[-2.], w1=[ 123.076]\n",
      "Stochastic Subgradient Descent(82/99): loss=921370181.6854671, w0=[-3.], w1=[-121.208]\n",
      "Stochastic Subgradient Descent(83/99): loss=597948340.8595628, w0=[-4.], w1=[-232.577]\n",
      "Stochastic Subgradient Descent(84/99): loss=625249946.9756835, w0=[-3.], w1=[-203.202]\n",
      "Stochastic Subgradient Descent(85/99): loss=484573423.8113001, w0=[-4.], w1=[-289.147]\n",
      "Stochastic Subgradient Descent(86/99): loss=884679079.8311977, w0=[-3.], w1=[-161.448]\n",
      "Stochastic Subgradient Descent(87/99): loss=520821796.3532858, w0=[-4.], w1=[-279.43]\n",
      "Stochastic Subgradient Descent(88/99): loss=394298145.4598805, w0=[-5.], w1=[-400.167]\n",
      "Stochastic Subgradient Descent(89/99): loss=386547484.0477992, w0=[-4.], w1=[-332.229]\n",
      "Stochastic Subgradient Descent(90/99): loss=448566950.24449885, w0=[-5.], w1=[-431.969]\n",
      "Stochastic Subgradient Descent(91/99): loss=385213948.6407252, w0=[-4.], w1=[-335.896]\n",
      "Stochastic Subgradient Descent(92/99): loss=428982740.03212327, w0=[-5.], w1=[-435.557]\n",
      "Stochastic Subgradient Descent(93/99): loss=684302694.0368322, w0=[-6.], w1=[-533.981]\n",
      "Stochastic Subgradient Descent(94/99): loss=397732463.24628204, w0=[-5.], w1=[-422.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Subgradient Descent(95/99): loss=810363326.8294797, w0=[-6.], w1=[-549.689]\n",
      "Stochastic Subgradient Descent(96/99): loss=380686911.1463355, w0=[-5.], w1=[-429.427]\n",
      "Stochastic Subgradient Descent(97/99): loss=459412666.15058506, w0=[-4.], w1=[-317.021]\n",
      "Stochastic Subgradient Descent(98/99): loss=334178281.1111742, w0=[-5.], w1=[-456.971]\n",
      "Stochastic Subgradient Descent(99/99): loss=244281560.8211903, w0=[-4.], w1=[-354.641]\n"
     ]
    }
   ],
   "source": [
    "weights, loss = stochastic_subgradient_descent_mae(y_train, x_train, initial_w, max_iters, gamma, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct: 152532 \n",
      "Total incorrect: 97468 \n",
      "Correct percentage: 61.0128 %\n"
     ]
    }
   ],
   "source": [
    "# Predict labels with found weights and print some useful information about quality of fit\n",
    "y_pred = predict_labels(weights, x_poly)\n",
    "correctness(yb, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and transform test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_path = \"../data/test.csv\"\n",
    "yb_test, input_data_test, ids_test = load_csv_data(test_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data_test = replace_nan_by_median(input_data_test, -999)\n",
    "x_submit_poly = build_advanced_poly(input_data_test, degree, important_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = predict_labels(weights, x_submit_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save predictions of test data in csv file, ready for the upload on kaggle\n",
    "create_csv_submission(ids_test, y_test_pred, \"test_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
