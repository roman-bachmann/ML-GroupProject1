{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding strongest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import itertools\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mlcomp import *\n",
    "from mlcomp.config import DATA_PATH\n",
    "from mlcomp.feature_eng import build_advanced_poly, build_simple_poly, build_mult_comb\n",
    "from mlcomp.helpers import split_data, compute_rmse, predict_labels\n",
    "from mlcomp.performance import correctness\n",
    "from mlcomp.data import load_csv_data, create_csv_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"Ridge regression using normal equations\"\"\"    \n",
    "    N = tx.shape[0]\n",
    "    D = tx.shape[1]\n",
    "    \n",
    "    inv_inner = np.dot(tx.T, tx) + 2 * N * lambda_ * np.identity(D)\n",
    "    inv = np.linalg.solve(inv_inner, np.identity(D))\n",
    "    w = np.dot(inv, np.dot(tx.T, y)).reshape((D, 1))\n",
    "    \n",
    "    rmse = compute_rmse(y, tx, w)\n",
    "    \n",
    "    return w, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_by_median(tx, nan_value):\n",
    "    \"\"\"Replaces values with a specified nan_value by the column median.\"\"\"\n",
    "    tx[tx == nan_value] = np.nan\n",
    "    col_median = np.nanmedian(tx, axis=0)\n",
    "    return np.where(np.isnan(tx), col_median, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_correlations(yb, tx):\n",
    "    \"\"\"Returns correlations of each input dimension with the labels,\n",
    "    as well as sorted indices of columns with strongest correlations.\"\"\"\n",
    "    cov_mat =  np.corrcoef(yb.T, tx.T)\n",
    "    corr_abs = abs(cov_mat[1:,0])\n",
    "    sorted_idxs = np.argsort(corr_abs)\n",
    "    return corr_abs, sorted_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_visualization(lambds, mse_tr, mse_te):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"cross_validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def get_train_indices(k_indices, k):\n",
    "    train_indices = np.array([])\n",
    "    for i in range(k_indices.shape[0]):\n",
    "        if (i != k-1):\n",
    "            train_indices = np.hstack((train_indices, k_indices[i]))\n",
    "    return train_indices.astype(int)\n",
    "\n",
    "def cross_validation_step(yb, tx, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    train_indices = get_train_indices(k_indices, k)\n",
    "    x_train = tx[train_indices]\n",
    "    y_train = yb[train_indices]\n",
    "    x_test = tx[k_indices[k-1]]\n",
    "    y_test = yb[k_indices[k-1]]\n",
    "    \n",
    "    weights, rmse = ridge_regression(y_train, x_train, lambda_)\n",
    "    \n",
    "    loss_tr = compute_rmse(y_train, x_train, weights)\n",
    "    loss_te = compute_rmse(y_test, x_test, weights)\n",
    "    \n",
    "    return loss_tr[0][0], loss_te[0][0]\n",
    "\n",
    "def cross_validation(yb, tx, k_fold, seed=1):\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(yb, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    best_rmse_te = float(\"inf\")\n",
    "    best_ind = 0\n",
    "    \n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        avg_loss_tr = 0\n",
    "        avg_loss_te = 0\n",
    "        for k in range(1, k_fold+1):\n",
    "            loss_tr, loss_te = cross_validation_step(yb, tx, k_indices, k, lambda_)\n",
    "            avg_loss_tr += loss_tr\n",
    "            avg_loss_te += loss_te\n",
    "        rmse_tr.append(avg_loss_tr/k_fold)\n",
    "        rmse_te.append(avg_loss_te/k_fold)\n",
    "        if (rmse_te[ind] < best_rmse_te):\n",
    "            best_rmse_te = rmse_te[ind]\n",
    "            best_ind = ind\n",
    "        print(\"lambda={l:.9f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "               l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "    \n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    return lambdas[best_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train.csv')\n",
    "yb, input_data, ids = load_csv_data(TRAIN_PATH, sub_sample=False)\n",
    "yb = yb.reshape((yb.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "tx = replace_nan_by_median(input_data, -999)\n",
    "degree = 5\n",
    "poly = PolynomialFeatures(degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_sim(x_train, y_train, x_test, y_test, seed=1):\n",
    "    lambdas = np.logspace(-4, 0, 5)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    best_rmse_te = float(\"inf\")\n",
    "    best_weights = 0\n",
    "    best_ind = 0\n",
    "    \n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        weights, rmse = ridge_regression(y_train, x_train, lambda_)\n",
    "        \n",
    "        rmse_tr.append(compute_rmse(y_train, x_train, weights))\n",
    "        rmse_te.append(compute_rmse(y_test, x_test, weights))\n",
    "        \n",
    "        if (rmse_te[ind] < best_rmse_te):\n",
    "            best_rmse_te = rmse_te[ind]\n",
    "            best_weights = weights\n",
    "            best_ind = ind\n",
    "        #print(\"lambda={l:.9f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "        #       l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "        \n",
    "    return lambdas[best_ind], best_weights, rmse_te[best_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness(yb, y_pred, verbose=False):\n",
    "    \"\"\"Takes inputs known y and predicted y and prints the ratio of correct predictions vs incorrect ones.\"\"\"\n",
    "    corrects = (y_pred == yb).sum()\n",
    "    perc = corrects / len(y_pred) * 100\n",
    "    if verbose:\n",
    "        incorrect = len(y_pred) - corrects\n",
    "        print(\"Total correct:\", corrects, \"\\nTotal incorrect:\", incorrect, \"\\nCorrect percentage:\", perc, \"%\")\n",
    "\n",
    "    return perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [0, 2, 7, 1, 11, 13, 5, 4, 10, 3, 6, 12, 9]\n",
    "seed = 12345\n",
    "ratio_split = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features --> [ 0  1  2  7 10 11 13 19]\n",
      "\tBest test loss features found: [ 0  1  2  7 10 11 13 19]\n",
      "\tBest lambda: 0.000278255940221\n",
      "\tCorrectness: 81.878\n",
      "\tLoss: [[ 179.6840572]]\n",
      "\tBest correctness features found: [ 0  1  2  7 10 11 13 19]\n",
      "\tBest lambda: 0.000278255940221\n",
      "\tCorrectness: 81.878\n",
      "\tLoss: [[ 179.6840572]]\n"
     ]
    }
   ],
   "source": [
    "best_features_te = []\n",
    "best_features_corr = []\n",
    "best_te_loss = float(\"inf\")\n",
    "best_correctness = 0\n",
    "\n",
    "#comb_iter = itertools.combinations(candidates, 8)\n",
    "comb_iter = itertools.combinations([0, 1, 2, 7, 10, 11, 13, 19], 8)\n",
    "\n",
    "for comb in comb_iter:\n",
    "    features = np.asarray(comb)\n",
    "    print(\"Features -->\", features)\n",
    "    \n",
    "    x_poly = poly.fit_transform(tx[:, features])\n",
    "    x_train, x_test, y_train, y_test = split_data(x_poly, yb, ratio_split, seed)\n",
    "    \n",
    "    best_lambda, w, loss_te = ridge_regression_sim(x_train, y_train, x_test, y_test, seed)\n",
    "\n",
    "    y_pred = predict_labels(w, x_poly)\n",
    "    correctness = correctness(yb, y_pred, verbose=False)\n",
    "    \n",
    "    if (loss_te < best_te_loss):\n",
    "        best_features_te = features\n",
    "        best_te_loss = loss_te\n",
    "        print(\"\\tBest test loss features found:\", best_features_te)\n",
    "        print(\"\\tBest lambda:\", best_lambda)\n",
    "        print(\"\\tCorrectness:\", correctness)\n",
    "        print(\"\\tLoss:\", loss_te)\n",
    "    if (correctness > best_correctness):\n",
    "        best_features_corr = features\n",
    "        best_correctness = correctness\n",
    "        print(\"\\tBest correctness features found:\", best_features_te)\n",
    "        print(\"\\tBest lambda:\", best_lambda)\n",
    "        print(\"\\tCorrectness:\", correctness)\n",
    "        print(\"\\tLoss:\", loss_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
